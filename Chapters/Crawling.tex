\section{Crawling}
Il \textit{crawling} é l'attivitá di scaricamento delle pagine web. Un \textit{crawler} é un dispositivo software che visita, scarica e analizza i contenuti delle pagine web a partire da un insieme di pagine dato, detto \textit{seme}. Il crawler procede nel suo processo di visita seguendo i collegamenti ipertestuali conenuti nelle varie pagine.\\
Le pagine web durante il processo di crawl si dividono in tre:
\begin{itemize}
    \item L'insieme delle pagine \textit{visitate}, \set{V}, che sono giá state scaricate e analizzate;
    \item La \textit{frontiera}, \set{F}, che é l'insieme delle pagine conosciute ma che non sono ancora state visitate;
    \item Línsieme \set{U} degli URL sconosciuti.
\end{itemize}
Le differenze tra l'attivitá di crawling e una banale visita all'interno di un grafo sono molto importanti, prima di tutto c'é il fatto che un crawl ha una dimensione ignota, non conosciamo $|V_G|$; secondariamente la frontiera é un enorme problema, in quanto la sua dimensione tende a crescere molto piú velocemente dell'insieme dei visitati.\\
In generale l'operazione di crawling parte caricando il seme in frontiera e, finché la frontiera non é vuota, viene estratto un URL dalla frontiera, secondo determinate politiche, l'URL viene visitato (e quindi scarica la pagina corrispondente), lo analizza derivandone nuovi URL tramite i collegamenti ipertestuali contenuti all'interno della pagina e sposta l'URL nell'insieme dei visitati. I nuovi URL vengono invece aggiunti alla frontiera se sono sconosciuti, e quindi non sono in \set{V \cup F}.\\
Diverse politiche di prioritizzazione della frontiera possono poi dare luogo ad approcci diversi al processo di crawling, posso per esempio estrarre prima degli URL a cui si arriva partendo da pagine che contengono determinate parole chiave.
\subsection{Il crivello}
Il crivello é la struttura dati di base di un crawler, questo accetta in ingresso URL potenzialmente da visitare e permette di prelevare URL pronti alla visita. Ogni URL viene estratto una e una sola volta in tutto il processo di crawling, indipendentemente da quante volte é stato inserito all'interno della struttura. In questo senso il crivello unisce le proprietá di un dizionario a quelle di una coda con prioritá e rappresenta al tempo stesso la frontiera, l'insieme dei visitati e la coda di visita. Combinare questi aspetti in una sola struttura é un lavoro complesso ma permette risparmi notevoli dal punto di vista pratico\footnote{Si noti che é possibile riordinare ulteriormente gli URL \textit{dopo} l'uscita dal crivello}.\\
Una prima osservazione é che spesso, per mantenere l'informazione di quali URL sono stati giá visitati (\set{V \cup F}) é preferibile sostituire gli URL con delle \textit{firme}, cioé con il risultato del calcolo di $h(u)$, dove h é una funzione di hash definita sulle stringhe e restituisce un hash di dimensione arbitraria, per esempio 64 bit. Questo ha due grandi benefici:
\begin{itemize}
    \item Risparmio di spazio non trascurabile, molti URL possono essere di grandi dimensioni e salvarli sempre in un centinaio\footnote{valore d'esempio arbitrario} si rivela una buona fonte di risparmio
    \item Uniformiamo le lunghezze degli URL a un valore standard
\end{itemize}
Il drawback di una soluzione del genere é che accettiamo il fatto che vi siano delle collisioni, é dunque possibile che due URL diversi vengano mappati sullo stesso valore di hash. Questo fenomeno é inevitabile, peró se abbiamo una funzione di hash che lavora su un numero di bit abbastanza grande, la probabilitá di incontrare una collisione sará cosí bassa da essere trascurabile.\\
Github.com implementa una soluzione del genere, viene impiegato SHA-1 (funzione di hash a 160bit) per calcolare un hash dell'URL di ciascuna delle repository nei loro database, la probabilitá di collisione é cosí bassa che é sostanzialmente impossibile. Adesso sembra che vogliano muoversi verso SHA-256.\\
Supponiamo ora di avere in memoria $n$ firme, la probabilitá che una nuova firma collida con una di quelle esistenti é $n/u$, dove $u$ é la dimensione dell'universo delle possibili firme. Nel caso di un sistema a 64 bit $u = 2^{64}$, e quindi possiamo memorizzare 100 miliardi di URL con una probabilitá di falsi positivi nell'ordine di $10^{11} / 2^{64} < 2^{37} / 2^{64} = 1 / 2^{27} < 1 / 10^8$, quindi avremo meno di un errore ogni 100 milioni di URL.